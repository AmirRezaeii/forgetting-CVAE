{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a661c0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-22T23:10:58.570979Z",
     "iopub.status.busy": "2025-10-22T23:10:58.570771Z",
     "iopub.status.idle": "2025-10-22T23:33:23.991242Z",
     "shell.execute_reply": "2025-10-22T23:33:23.990328Z"
    },
    "id": "qr_prdZvgcUp",
    "outputId": "3f03cea2-9e30-4796-c228-9a8ce98fa51a",
    "papermill": {
     "duration": 1345.424948,
     "end_time": "2025-10-22T23:33:23.992679",
     "exception": false,
     "start_time": "2025-10-22T23:10:58.567731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Configuration:\n",
      "Digit to forget: [0]\n",
      "Lambda (EWC): 100\n",
      "Gamma: 1\n",
      "Forgetting iterations: 10000\n",
      "Load existing models: True\n",
      "Output directory: ./results/forgetting_test_0\n",
      "No saved model found at: ./saved_models/original_vae.pt\n",
      "Training original CVAE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.55MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 120kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.14MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.47MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 70412.649967\n",
      "Epoch 10, Loss: 32287.901400\n",
      "Epoch 20, Loss: 29604.869199\n",
      "Epoch 30, Loss: 28381.453437\n",
      "Epoch 40, Loss: 27651.993605\n",
      "Model saved to: ./saved_models/original_vae.pt\n",
      "No saved Fisher matrix found at: ./saved_models/fisher_matrix.pkl\n",
      "Calculating FIM\n",
      "Fisher matrix saved to: ./saved_models/fisher_matrix.pkl\n",
      "Generating original samples\n",
      "No saved model found at: ./saved_models/forgotten_vae_digit_[0].pt\n",
      "Training to forget [0]\n",
      "Forgetting Step: 1000, Loss: 175461.125000\n",
      "Forgetting Step: 2000, Loss: 173720.015625\n",
      "Forgetting Step: 3000, Loss: 172912.875000\n",
      "Forgetting Step: 4000, Loss: 172548.984375\n",
      "Forgetting Step: 5000, Loss: 171329.453125\n",
      "Forgetting Step: 6000, Loss: 171668.812500\n",
      "Forgetting Step: 7000, Loss: 171866.671875\n",
      "Forgetting Step: 8000, Loss: 171814.656250\n",
      "Forgetting Step: 9000, Loss: 171682.062500\n",
      "Forgetting Step: 10000, Loss: 169973.281250\n",
      "Model saved to: ./saved_models/forgotten_vae_digits_0.pt\n",
      "Generating forgotten samples\n",
      "No saved classifier found at: ./saved_models/classifier.pt\n",
      "Training classifier\n",
      "Classifier saved to: ./saved_models/classifier.pt\n",
      "Evaluating samples with classifier\n",
      "\n",
      "RESULTS SUMMARY\n",
      "\n",
      "All results and visualizations saved to: ./results/forgetting_test_0\n",
      "All models saved to: ./saved_models\n",
      "Using device: cuda\n",
      "Configuration:\n",
      "Digit to forget: [1]\n",
      "Lambda (EWC): 100\n",
      "Gamma: 1\n",
      "Forgetting iterations: 10000\n",
      "Load existing models: True\n",
      "Output directory: ./results/forgetting_test_1\n",
      "Loading model from: ./saved_models/original_vae.pt\n",
      "Using pre-trained original VAE\n",
      "Loading Fisher matrix from: ./saved_models/fisher_matrix.pkl\n",
      "Using pre-calculated Fisher matrix\n",
      "Generating original samples\n",
      "No saved model found at: ./saved_models/forgotten_vae_digit_[1].pt\n",
      "Training to forget [1]\n",
      "Forgetting Step: 1000, Loss: 176162.734375\n",
      "Forgetting Step: 2000, Loss: 174647.937500\n",
      "Forgetting Step: 3000, Loss: 173985.296875\n",
      "Forgetting Step: 4000, Loss: 173429.296875\n",
      "Forgetting Step: 5000, Loss: 172687.281250\n",
      "Forgetting Step: 6000, Loss: 172663.218750\n",
      "Forgetting Step: 7000, Loss: 173234.578125\n",
      "Forgetting Step: 8000, Loss: 172104.656250\n",
      "Forgetting Step: 9000, Loss: 172433.562500\n",
      "Forgetting Step: 10000, Loss: 172356.500000\n",
      "Model saved to: ./saved_models/forgotten_vae_digits_1.pt\n",
      "Generating forgotten samples\n",
      "Loading classifier from: ./saved_models/classifier.pt\n",
      "Using pre-trained classifier\n",
      "Evaluating samples with classifier\n",
      "\n",
      "RESULTS SUMMARY\n",
      "\n",
      "All results and visualizations saved to: ./results/forgetting_test_1\n",
      "All models saved to: ./saved_models\n",
      "Using device: cuda\n",
      "Configuration:\n",
      "Digit to forget: [2]\n",
      "Lambda (EWC): 100\n",
      "Gamma: 1\n",
      "Forgetting iterations: 10000\n",
      "Load existing models: True\n",
      "Output directory: ./results/forgetting_test_2\n",
      "Loading model from: ./saved_models/original_vae.pt\n",
      "Using pre-trained original VAE\n",
      "Loading Fisher matrix from: ./saved_models/fisher_matrix.pkl\n",
      "Using pre-calculated Fisher matrix\n",
      "Generating original samples\n",
      "No saved model found at: ./saved_models/forgotten_vae_digit_[2].pt\n",
      "Training to forget [2]\n",
      "Forgetting Step: 1000, Loss: 175619.812500\n",
      "Forgetting Step: 2000, Loss: 172941.578125\n",
      "Forgetting Step: 3000, Loss: 172641.812500\n",
      "Forgetting Step: 4000, Loss: 172084.968750\n",
      "Forgetting Step: 5000, Loss: 171668.140625\n",
      "Forgetting Step: 6000, Loss: 171934.968750\n",
      "Forgetting Step: 7000, Loss: 170613.984375\n",
      "Forgetting Step: 8000, Loss: 172203.078125\n",
      "Forgetting Step: 9000, Loss: 171623.015625\n",
      "Forgetting Step: 10000, Loss: 171258.046875\n",
      "Model saved to: ./saved_models/forgotten_vae_digits_2.pt\n",
      "Generating forgotten samples\n",
      "Loading classifier from: ./saved_models/classifier.pt\n",
      "Using pre-trained classifier\n",
      "Evaluating samples with classifier\n",
      "\n",
      "RESULTS SUMMARY\n",
      "\n",
      "All results and visualizations saved to: ./results/forgetting_test_2\n",
      "All models saved to: ./saved_models\n",
      "Using device: cuda\n",
      "Configuration:\n",
      "Digit to forget: [3]\n",
      "Lambda (EWC): 100\n",
      "Gamma: 1\n",
      "Forgetting iterations: 10000\n",
      "Load existing models: True\n",
      "Output directory: ./results/forgetting_test_3\n",
      "Loading model from: ./saved_models/original_vae.pt\n",
      "Using pre-trained original VAE\n",
      "Loading Fisher matrix from: ./saved_models/fisher_matrix.pkl\n",
      "Using pre-calculated Fisher matrix\n",
      "Generating original samples\n",
      "No saved model found at: ./saved_models/forgotten_vae_digit_[3].pt\n",
      "Training to forget [3]\n",
      "Forgetting Step: 1000, Loss: 176056.531250\n",
      "Forgetting Step: 2000, Loss: 174279.421875\n",
      "Forgetting Step: 3000, Loss: 173286.312500\n",
      "Forgetting Step: 4000, Loss: 173764.828125\n",
      "Forgetting Step: 5000, Loss: 171921.078125\n",
      "Forgetting Step: 6000, Loss: 171738.156250\n",
      "Forgetting Step: 7000, Loss: 172390.734375\n",
      "Forgetting Step: 8000, Loss: 171367.890625\n",
      "Forgetting Step: 9000, Loss: 171444.562500\n",
      "Forgetting Step: 10000, Loss: 171749.031250\n",
      "Model saved to: ./saved_models/forgotten_vae_digits_3.pt\n",
      "Generating forgotten samples\n",
      "Loading classifier from: ./saved_models/classifier.pt\n",
      "Using pre-trained classifier\n",
      "Evaluating samples with classifier\n",
      "\n",
      "RESULTS SUMMARY\n",
      "\n",
      "All results and visualizations saved to: ./results/forgetting_test_3\n",
      "All models saved to: ./saved_models\n",
      "Using device: cuda\n",
      "Configuration:\n",
      "Digit to forget: [4]\n",
      "Lambda (EWC): 100\n",
      "Gamma: 1\n",
      "Forgetting iterations: 10000\n",
      "Load existing models: True\n",
      "Output directory: ./results/forgetting_test_4\n",
      "Loading model from: ./saved_models/original_vae.pt\n",
      "Using pre-trained original VAE\n",
      "Loading Fisher matrix from: ./saved_models/fisher_matrix.pkl\n",
      "Using pre-calculated Fisher matrix\n",
      "Generating original samples\n",
      "No saved model found at: ./saved_models/forgotten_vae_digit_[4].pt\n",
      "Training to forget [4]\n",
      "Forgetting Step: 1000, Loss: 175243.406250\n",
      "Forgetting Step: 2000, Loss: 173141.921875\n",
      "Forgetting Step: 3000, Loss: 172924.593750\n",
      "Forgetting Step: 4000, Loss: 172699.703125\n",
      "Forgetting Step: 5000, Loss: 171248.515625\n",
      "Forgetting Step: 6000, Loss: 170961.031250\n",
      "Forgetting Step: 7000, Loss: 172169.406250\n",
      "Forgetting Step: 8000, Loss: 170992.625000\n",
      "Forgetting Step: 9000, Loss: 171239.140625\n",
      "Forgetting Step: 10000, Loss: 170834.296875\n",
      "Model saved to: ./saved_models/forgotten_vae_digits_4.pt\n",
      "Generating forgotten samples\n",
      "Loading classifier from: ./saved_models/classifier.pt\n",
      "Using pre-trained classifier\n",
      "Evaluating samples with classifier\n",
      "\n",
      "RESULTS SUMMARY\n",
      "\n",
      "All results and visualizations saved to: ./results/forgetting_test_4\n",
      "All models saved to: ./saved_models\n",
      "Using device: cuda\n",
      "Configuration:\n",
      "Digit to forget: [5]\n",
      "Lambda (EWC): 100\n",
      "Gamma: 1\n",
      "Forgetting iterations: 10000\n",
      "Load existing models: True\n",
      "Output directory: ./results/forgetting_test_5\n",
      "Loading model from: ./saved_models/original_vae.pt\n",
      "Using pre-trained original VAE\n",
      "Loading Fisher matrix from: ./saved_models/fisher_matrix.pkl\n",
      "Using pre-calculated Fisher matrix\n",
      "Generating original samples\n",
      "No saved model found at: ./saved_models/forgotten_vae_digit_[5].pt\n",
      "Training to forget [5]\n",
      "Forgetting Step: 1000, Loss: 175253.437500\n",
      "Forgetting Step: 2000, Loss: 173472.734375\n",
      "Forgetting Step: 3000, Loss: 172885.859375\n",
      "Forgetting Step: 4000, Loss: 172054.968750\n",
      "Forgetting Step: 5000, Loss: 172028.765625\n",
      "Forgetting Step: 6000, Loss: 171483.171875\n",
      "Forgetting Step: 7000, Loss: 171140.656250\n",
      "Forgetting Step: 8000, Loss: 171067.078125\n",
      "Forgetting Step: 9000, Loss: 171530.859375\n",
      "Forgetting Step: 10000, Loss: 172173.171875\n",
      "Model saved to: ./saved_models/forgotten_vae_digits_5.pt\n",
      "Generating forgotten samples\n",
      "Loading classifier from: ./saved_models/classifier.pt\n",
      "Using pre-trained classifier\n",
      "Evaluating samples with classifier\n",
      "\n",
      "RESULTS SUMMARY\n",
      "\n",
      "All results and visualizations saved to: ./results/forgetting_test_5\n",
      "All models saved to: ./saved_models\n",
      "Using device: cuda\n",
      "Configuration:\n",
      "Digit to forget: [6]\n",
      "Lambda (EWC): 100\n",
      "Gamma: 1\n",
      "Forgetting iterations: 10000\n",
      "Load existing models: True\n",
      "Output directory: ./results/forgetting_test_6\n",
      "Loading model from: ./saved_models/original_vae.pt\n",
      "Using pre-trained original VAE\n",
      "Loading Fisher matrix from: ./saved_models/fisher_matrix.pkl\n",
      "Using pre-calculated Fisher matrix\n",
      "Generating original samples\n",
      "No saved model found at: ./saved_models/forgotten_vae_digit_[6].pt\n",
      "Training to forget [6]\n",
      "Forgetting Step: 1000, Loss: 174772.609375\n",
      "Forgetting Step: 2000, Loss: 174209.078125\n",
      "Forgetting Step: 3000, Loss: 172168.765625\n",
      "Forgetting Step: 4000, Loss: 172668.250000\n",
      "Forgetting Step: 5000, Loss: 172794.406250\n",
      "Forgetting Step: 6000, Loss: 171399.078125\n",
      "Forgetting Step: 7000, Loss: 172346.171875\n",
      "Forgetting Step: 8000, Loss: 171245.609375\n",
      "Forgetting Step: 9000, Loss: 170713.890625\n",
      "Forgetting Step: 10000, Loss: 171453.437500\n",
      "Model saved to: ./saved_models/forgotten_vae_digits_6.pt\n",
      "Generating forgotten samples\n",
      "Loading classifier from: ./saved_models/classifier.pt\n",
      "Using pre-trained classifier\n",
      "Evaluating samples with classifier\n",
      "\n",
      "RESULTS SUMMARY\n",
      "\n",
      "All results and visualizations saved to: ./results/forgetting_test_6\n",
      "All models saved to: ./saved_models\n",
      "Using device: cuda\n",
      "Configuration:\n",
      "Digit to forget: [7]\n",
      "Lambda (EWC): 100\n",
      "Gamma: 1\n",
      "Forgetting iterations: 10000\n",
      "Load existing models: True\n",
      "Output directory: ./results/forgetting_test_7\n",
      "Loading model from: ./saved_models/original_vae.pt\n",
      "Using pre-trained original VAE\n",
      "Loading Fisher matrix from: ./saved_models/fisher_matrix.pkl\n",
      "Using pre-calculated Fisher matrix\n",
      "Generating original samples\n",
      "No saved model found at: ./saved_models/forgotten_vae_digit_[7].pt\n",
      "Training to forget [7]\n",
      "Forgetting Step: 1000, Loss: 175020.609375\n",
      "Forgetting Step: 2000, Loss: 173774.468750\n",
      "Forgetting Step: 3000, Loss: 172806.625000\n",
      "Forgetting Step: 4000, Loss: 172103.453125\n",
      "Forgetting Step: 5000, Loss: 171574.125000\n",
      "Forgetting Step: 6000, Loss: 171700.406250\n",
      "Forgetting Step: 7000, Loss: 172423.125000\n",
      "Forgetting Step: 8000, Loss: 170629.437500\n",
      "Forgetting Step: 9000, Loss: 171248.484375\n",
      "Forgetting Step: 10000, Loss: 171623.343750\n",
      "Model saved to: ./saved_models/forgotten_vae_digits_7.pt\n",
      "Generating forgotten samples\n",
      "Loading classifier from: ./saved_models/classifier.pt\n",
      "Using pre-trained classifier\n",
      "Evaluating samples with classifier\n",
      "\n",
      "RESULTS SUMMARY\n",
      "\n",
      "All results and visualizations saved to: ./results/forgetting_test_7\n",
      "All models saved to: ./saved_models\n",
      "Using device: cuda\n",
      "Configuration:\n",
      "Digit to forget: [8]\n",
      "Lambda (EWC): 100\n",
      "Gamma: 1\n",
      "Forgetting iterations: 10000\n",
      "Load existing models: True\n",
      "Output directory: ./results/forgetting_test_8\n",
      "Loading model from: ./saved_models/original_vae.pt\n",
      "Using pre-trained original VAE\n",
      "Loading Fisher matrix from: ./saved_models/fisher_matrix.pkl\n",
      "Using pre-calculated Fisher matrix\n",
      "Generating original samples\n",
      "No saved model found at: ./saved_models/forgotten_vae_digit_[8].pt\n",
      "Training to forget [8]\n",
      "Forgetting Step: 1000, Loss: 174558.812500\n",
      "Forgetting Step: 2000, Loss: 174001.500000\n",
      "Forgetting Step: 3000, Loss: 172905.406250\n",
      "Forgetting Step: 4000, Loss: 171439.953125\n",
      "Forgetting Step: 5000, Loss: 171883.453125\n",
      "Forgetting Step: 6000, Loss: 170901.031250\n",
      "Forgetting Step: 7000, Loss: 170437.765625\n",
      "Forgetting Step: 8000, Loss: 171147.140625\n",
      "Forgetting Step: 9000, Loss: 172053.953125\n",
      "Forgetting Step: 10000, Loss: 170336.921875\n",
      "Model saved to: ./saved_models/forgotten_vae_digits_8.pt\n",
      "Generating forgotten samples\n",
      "Loading classifier from: ./saved_models/classifier.pt\n",
      "Using pre-trained classifier\n",
      "Evaluating samples with classifier\n",
      "\n",
      "RESULTS SUMMARY\n",
      "\n",
      "All results and visualizations saved to: ./results/forgetting_test_8\n",
      "All models saved to: ./saved_models\n",
      "Using device: cuda\n",
      "Configuration:\n",
      "Digit to forget: [9]\n",
      "Lambda (EWC): 100\n",
      "Gamma: 1\n",
      "Forgetting iterations: 10000\n",
      "Load existing models: True\n",
      "Output directory: ./results/forgetting_test_9\n",
      "Loading model from: ./saved_models/original_vae.pt\n",
      "Using pre-trained original VAE\n",
      "Loading Fisher matrix from: ./saved_models/fisher_matrix.pkl\n",
      "Using pre-calculated Fisher matrix\n",
      "Generating original samples\n",
      "No saved model found at: ./saved_models/forgotten_vae_digit_[9].pt\n",
      "Training to forget [9]\n",
      "Forgetting Step: 1000, Loss: 175985.937500\n",
      "Forgetting Step: 2000, Loss: 173928.281250\n",
      "Forgetting Step: 3000, Loss: 173902.328125\n",
      "Forgetting Step: 4000, Loss: 172909.203125\n",
      "Forgetting Step: 5000, Loss: 172338.562500\n",
      "Forgetting Step: 6000, Loss: 171439.093750\n",
      "Forgetting Step: 7000, Loss: 171184.250000\n",
      "Forgetting Step: 8000, Loss: 171746.312500\n",
      "Forgetting Step: 9000, Loss: 171825.921875\n",
      "Forgetting Step: 10000, Loss: 170797.109375\n",
      "Model saved to: ./saved_models/forgotten_vae_digits_9.pt\n",
      "Generating forgotten samples\n",
      "Loading classifier from: ./saved_models/classifier.pt\n",
      "Using pre-trained classifier\n",
      "Evaluating samples with classifier\n",
      "\n",
      "RESULTS SUMMARY\n",
      "\n",
      "All results and visualizations saved to: ./results/forgetting_test_9\n",
      "All models saved to: ./saved_models\n",
      "Using device: cuda\n",
      "Configuration:\n",
      "Digit to forget: [3, 7]\n",
      "Lambda (EWC): 100\n",
      "Gamma: 1\n",
      "Forgetting iterations: 10000\n",
      "Load existing models: True\n",
      "Output directory: ./results/forgetting_test_3_7\n",
      "Loading model from: ./saved_models/original_vae.pt\n",
      "Using pre-trained original VAE\n",
      "Loading Fisher matrix from: ./saved_models/fisher_matrix.pkl\n",
      "Using pre-calculated Fisher matrix\n",
      "Generating original samples\n",
      "No saved model found at: ./saved_models/forgotten_vae_digit_[3, 7].pt\n",
      "Training to forget [3, 7]\n",
      "Forgetting Step: 1000, Loss: 175352.578125\n",
      "Forgetting Step: 2000, Loss: 174428.062500\n",
      "Forgetting Step: 3000, Loss: 173750.265625\n",
      "Forgetting Step: 4000, Loss: 172754.125000\n",
      "Forgetting Step: 5000, Loss: 172146.265625\n",
      "Forgetting Step: 6000, Loss: 172293.031250\n",
      "Forgetting Step: 7000, Loss: 172130.390625\n",
      "Forgetting Step: 8000, Loss: 172329.687500\n",
      "Forgetting Step: 9000, Loss: 172164.015625\n",
      "Forgetting Step: 10000, Loss: 171854.578125\n",
      "Model saved to: ./saved_models/forgotten_vae_digits_3_7.pt\n",
      "Generating forgotten samples\n",
      "Loading classifier from: ./saved_models/classifier.pt\n",
      "Using pre-trained classifier\n",
      "Evaluating samples with classifier\n",
      "\n",
      "RESULTS SUMMARY\n",
      "\n",
      "All results and visualizations saved to: ./results/forgetting_test_3_7\n",
      "All models saved to: ./saved_models\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import logging\n",
    "import copy\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "class OneHotCVAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim, class_size=10):\n",
    "        super(OneHotCVAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(x_dim + class_size, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc4 = nn.Linear(z_dim + class_size, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "\n",
    "    def encoder(self, x, c):\n",
    "        inputs = torch.cat([x,c], dim=1)\n",
    "        h = F.relu(self.fc1(inputs))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h)\n",
    "\n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decoder(self, z, c):\n",
    "        inputs = torch.cat([z,c], dim=1)\n",
    "        h = F.relu(self.fc4(inputs))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return torch.sigmoid(self.fc6(h))\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, log_var = self.encoder(x.view(-1, 784), c)\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z, c), mu, log_var\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.bn1 = nn.BatchNorm2d(10)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.bn2 = nn.BatchNorm2d(20)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.bn1(self.conv1(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.bn2(self.conv2(x))), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, forget=[0]):\n",
    "        self.dataset = \"MNIST\"\n",
    "        self.x_dim = 784\n",
    "        self.h_dim1 = 512\n",
    "        self.h_dim2 = 256\n",
    "        self.z_dim = 8\n",
    "        if isinstance(forget, int):\n",
    "            forget = [forget]\n",
    "        self.digit_to_forget = forget\n",
    "        self.lmbda = 100\n",
    "        self.gamma = 1\n",
    "        self.n_forget_iters = 10000\n",
    "        self.batch_size = 256\n",
    "        self.lr = 1e-4\n",
    "        self.n_samples_per_class = 10\n",
    "        self.load_existing_models = True\n",
    "        self.base_model_dir = \"./saved_models\"\n",
    "        forget_label = \"_\".join(map(str, forget)) if isinstance(forget, (list, tuple)) else str(forget)\n",
    "        self.exp_root_dir = f\"./results/forgetting_test_{forget_label}\"\n",
    "        self.log_dir = os.path.join(self.exp_root_dir, 'logs')\n",
    "        self.ckpt_dir = os.path.join(self.exp_root_dir, 'ckpts')\n",
    "        self.sample_dir = os.path.join(self.exp_root_dir, 'samples')\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        os.makedirs(self.ckpt_dir, exist_ok=True)\n",
    "        os.makedirs(self.sample_dir, exist_ok=True)\n",
    "        os.makedirs(self.base_model_dir, exist_ok=True)\n",
    "\n",
    "def save_model(model, filename, config=None):\n",
    "    save_path = os.path.join(config.base_model_dir, filename)\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': config\n",
    "    }, save_path)\n",
    "    print(f\"Model saved to: {save_path}\")\n",
    "\n",
    "def load_model(model_class, filename, config, device):\n",
    "    load_path = os.path.join(config.base_model_dir, filename)\n",
    "    if os.path.exists(load_path):\n",
    "        print(f\"Loading model from: {load_path}\")\n",
    "        try:\n",
    "            checkpoint = torch.load(load_path, map_location=device, weights_only=True)\n",
    "        except:\n",
    "            checkpoint = torch.load(load_path, map_location=device, weights_only=False)\n",
    "        model = model_class(config.x_dim, config.h_dim1, config.h_dim2, config.z_dim).to(device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        return model\n",
    "    else:\n",
    "        print(f\"No saved model found at: {load_path}\")\n",
    "        return None\n",
    "\n",
    "def save_fisher_matrix(fisher_dict, config):\n",
    "    fisher_path = os.path.join(config.base_model_dir, 'fisher_matrix.pkl')\n",
    "    with open(fisher_path, 'wb') as f:\n",
    "        pickle.dump(fisher_dict, f)\n",
    "    print(f\"Fisher matrix saved to: {fisher_path}\")\n",
    "\n",
    "def load_fisher_matrix(config):\n",
    "    fisher_path = os.path.join(config.base_model_dir, 'fisher_matrix.pkl')\n",
    "    if os.path.exists(fisher_path):\n",
    "        print(f\"Loading Fisher matrix from: {fisher_path}\")\n",
    "        with open(fisher_path, 'rb') as f:\n",
    "            fisher_dict = pickle.load(f)\n",
    "        return fisher_dict\n",
    "    else:\n",
    "        print(f\"No saved Fisher matrix found at: {fisher_path}\")\n",
    "        return None\n",
    "\n",
    "def save_classifier(classifier, config):\n",
    "    classifier_path = os.path.join(config.base_model_dir, 'classifier.pt')\n",
    "    torch.save(classifier.state_dict(), classifier_path)\n",
    "    print(f\"Classifier saved to: {classifier_path}\")\n",
    "\n",
    "def load_classifier(config, device):\n",
    "    classifier_path = os.path.join(config.base_model_dir, 'classifier.pt')\n",
    "    if os.path.exists(classifier_path):\n",
    "        print(f\"Loading classifier from: {classifier_path}\")\n",
    "        classifier = Classifier().to(device)\n",
    "        classifier.load_state_dict(torch.load(classifier_path, map_location=device, weights_only=True))\n",
    "        return classifier\n",
    "    else:\n",
    "        print(f\"No saved classifier found at: {classifier_path}\")\n",
    "        return None\n",
    "\n",
    "def train_cvae(config, device):\n",
    "    print(\"Training original CVAE\")\n",
    "    train_dataset = datasets.MNIST('./dataset', train=True, download=True,\n",
    "                                   transform=transforms.ToTensor())\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    vae = OneHotCVAE(config.x_dim, config.h_dim1, config.h_dim2, config.z_dim).to(device)\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=config.lr)\n",
    "    vae.train()\n",
    "    for epoch in range(50):\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            c = F.one_hot(labels, 10).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, log_var = vae(data, c)\n",
    "            loss = loss_function(recon_batch, data, mu, log_var)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {train_loss/len(train_loader):.6f}')\n",
    "    save_model(vae, 'original_vae.pt', config)\n",
    "    return vae\n",
    "\n",
    "def calculate_fim(vae, config, device):\n",
    "    print(\"Calculating FIM\")\n",
    "    fisher_dict = {}\n",
    "    for name, param in vae.named_parameters():\n",
    "        fisher_dict[name] = torch.zeros_like(param.data)\n",
    "    n_fim_samples = 1000\n",
    "    for _ in range(n_fim_samples):\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(1, config.z_dim).to(device)\n",
    "            c = torch.randint(0, 10, (1,)).to(device)\n",
    "            c = F.one_hot(c, 10)\n",
    "            sample = vae.decoder(z, c)\n",
    "        vae.zero_grad()\n",
    "        recon_batch, mu, log_var = vae(sample, c)\n",
    "        loss = loss_function(recon_batch, sample, mu, log_var)\n",
    "        loss.backward()\n",
    "        for name, param in vae.named_parameters():\n",
    "            fisher_dict[name] += (param.grad.data ** 2) / n_fim_samples\n",
    "    save_fisher_matrix(fisher_dict, config)\n",
    "    return fisher_dict\n",
    "\n",
    "def train_forgetting(original_vae, fisher_dict, config, device):\n",
    "    print(f\"Training to forget {config.digit_to_forget}\")\n",
    "    vae = copy.deepcopy(original_vae)\n",
    "    vae.train()\n",
    "    params_mle_dict = {}\n",
    "    for name, param in original_vae.named_parameters():\n",
    "        params_mle_dict[name] = param.data.clone()\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=config.lr)\n",
    "    label_choices = [d for d in range(10) if d not in config.digit_to_forget]\n",
    "    vae_clone = copy.deepcopy(vae)\n",
    "    vae_clone.eval()\n",
    "    losses = []\n",
    "    for step in range(config.n_forget_iters):\n",
    "        c_remember = torch.from_numpy(np.random.choice(label_choices, size=config.batch_size)).to(device)\n",
    "        c_remember = F.one_hot(c_remember, 10)\n",
    "        z_remember = torch.randn((config.batch_size, config.z_dim)).to(device)\n",
    "        forget_labels = np.random.choice(config.digit_to_forget, size=config.batch_size)\n",
    "        c_forget = torch.from_numpy(forget_labels).to(device)\n",
    "        c_forget = F.one_hot(c_forget, 10)\n",
    "        out_forget = torch.rand((config.batch_size, 1, 28, 28)).to(device)\n",
    "        with torch.no_grad():\n",
    "            out_remember = vae_clone.decoder(z_remember, c_remember).view(-1, 1, 28, 28)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, log_var = vae(out_forget, c_forget)\n",
    "        loss = loss_function(recon_batch, out_forget, mu, log_var)\n",
    "        recon_batch, mu, log_var = vae(out_remember, c_remember)\n",
    "        loss += config.gamma * loss_function(recon_batch, out_remember, mu, log_var)\n",
    "        for n, p in vae.named_parameters():\n",
    "            _loss = fisher_dict[n].to(device) * (p - params_mle_dict[n].to(device)) ** 2\n",
    "            loss += config.lmbda * _loss.sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        if (step + 1) % 1000 == 0:\n",
    "            print(f'Forgetting Step: {step+1}, Loss: {loss.item():.6f}')\n",
    "    forgotten_model_name = f'forgotten_vae_digits_{\"_\".join(map(str, config.digit_to_forget))}.pt'\n",
    "    save_model(vae, forgotten_model_name, config)\n",
    "    return vae, losses\n",
    "\n",
    "def generate_samples(vae, config, device, prefix=\"original\"):\n",
    "    print(f\"Generating {prefix} samples\")\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        for digit in range(10):\n",
    "            z = torch.randn((config.n_samples_per_class, config.z_dim)).to(device)\n",
    "            c = (torch.ones(config.n_samples_per_class, dtype=int) * digit).to(device)\n",
    "            c = F.one_hot(c, 10)\n",
    "            samples = vae.decoder(z, c).view(-1, 1, 28, 28)\n",
    "            digit_dir = os.path.join(config.sample_dir, prefix, f'digit_{digit}')\n",
    "            os.makedirs(digit_dir, exist_ok=True)\n",
    "            for i, sample in enumerate(samples):\n",
    "                save_image(sample, os.path.join(digit_dir, f'sample_{i}.png'))\n",
    "            grid = make_grid(samples, nrow=5)\n",
    "            save_image(grid, os.path.join(config.sample_dir, f'{prefix}_digit_{digit}_grid.png'))\n",
    "\n",
    "\n",
    "def train_classifier(config, device):\n",
    "    print(\"Training classifier\")\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./dataset', train=True, download=True,\n",
    "                      transform=transforms.ToTensor()),\n",
    "        batch_size=64, shuffle=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./dataset', train=False, download=True,\n",
    "                      transform=transforms.ToTensor()),\n",
    "        batch_size=1000, shuffle=True)\n",
    "    classifier = Classifier().to(device)\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=1e-4)\n",
    "    classifier.train()\n",
    "    for epoch in range(5):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = classifier(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    save_classifier(classifier, config)\n",
    "    return classifier\n",
    "\n",
    "def evaluate_with_classifier(classifier, sample_dir, device):\n",
    "    print(\"Evaluating samples with classifier\")\n",
    "    results = {}\n",
    "    for digit in range(10):\n",
    "        original_samples = []\n",
    "        forgotten_samples = []\n",
    "        orig_dir = os.path.join(sample_dir, 'original', f'digit_{digit}')\n",
    "        forg_dir = os.path.join(sample_dir, 'forgotten', f'digit_{digit}')\n",
    "        if os.path.exists(orig_dir):\n",
    "            for img_file in os.listdir(orig_dir)[:10]:\n",
    "                img = Image.open(os.path.join(orig_dir, img_file)).convert('L')\n",
    "                img_tensor = transforms.ToTensor()(img).unsqueeze(0).to(device)\n",
    "                original_samples.append(img_tensor)\n",
    "        if os.path.exists(forg_dir):\n",
    "            for img_file in os.listdir(forg_dir)[:10]:\n",
    "                img = Image.open(os.path.join(forg_dir, img_file)).convert('L')\n",
    "                img_tensor = transforms.ToTensor()(img).unsqueeze(0).to(device)\n",
    "                forgotten_samples.append(img_tensor)\n",
    "\n",
    "        if original_samples:\n",
    "            orig_tensor = torch.cat(original_samples)\n",
    "            with torch.no_grad():\n",
    "                orig_log_probs = classifier(orig_tensor)\n",
    "                orig_probs = torch.exp(orig_log_probs)\n",
    "                orig_entropy = -torch.sum(orig_probs * orig_log_probs, dim=1).mean().item()\n",
    "                orig_target_prob = orig_probs[:, digit].mean().item()\n",
    "            results[f'original_digit_{digit}'] = {\n",
    "                'entropy': orig_entropy,\n",
    "                'target_prob': orig_target_prob\n",
    "            }\n",
    "\n",
    "        if forgotten_samples:\n",
    "            forg_tensor = torch.cat(forgotten_samples)\n",
    "            with torch.no_grad():\n",
    "                forg_log_probs = classifier(forg_tensor)\n",
    "                forg_probs = torch.exp(forg_log_probs)\n",
    "                forg_entropy = -torch.sum(forg_probs * forg_log_probs, dim=1).mean().item()\n",
    "                forg_target_prob = forg_probs[:, digit].mean().item()\n",
    "            results[f'forgotten_digit_{digit}'] = {\n",
    "                'entropy': forg_entropy,\n",
    "                'target_prob': forg_target_prob\n",
    "            }\n",
    "    return results\n",
    "\n",
    "def create_comparison_plots(results, config):\n",
    "    if hasattr(config, \"digits_to_forget\"):\n",
    "        forgotten_digits = config.digits_to_forget\n",
    "    else:\n",
    "        forgotten_digits = [config.digit_to_forget]\n",
    "\n",
    "    original_entropies = []\n",
    "    forgotten_entropies = []\n",
    "    digits = []\n",
    "    for digit in range(10):\n",
    "        orig_key = f'original_digit_{digit}'\n",
    "        forg_key = f'forgotten_digit_{digit}'\n",
    "        if orig_key in results and forg_key in results:\n",
    "            original_entropies.append(results[orig_key]['entropy'])\n",
    "            forgotten_entropies.append(results[forg_key]['entropy'])\n",
    "            digits.append(digit)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    x = np.arange(len(digits))\n",
    "    width = 0.35\n",
    "    plt.bar(x - width/2, original_entropies, width, label='Original', alpha=0.7)\n",
    "    plt.bar(x + width/2, forgotten_entropies, width, label='Forgotten', alpha=0.7)\n",
    "    for fd in forgotten_digits:\n",
    "        if fd in digits:\n",
    "            plt.axvline(x=digits.index(fd), color='red', linestyle='--')\n",
    "    plt.xlabel('Digit')\n",
    "    plt.ylabel('Entropy')\n",
    "    plt.title('Entropy Comparison')\n",
    "    plt.xticks(x, digits)\n",
    "    plt.legend([f'Forgotten Digits: {forgotten_digits}'])\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    original_probs = [results[f'original_digit_{d}']['target_prob'] for d in digits]\n",
    "    forgotten_probs = [results[f'forgotten_digit_{d}']['target_prob'] for d in digits]\n",
    "    plt.bar(x - width/2, original_probs, width, label='Original', alpha=0.7)\n",
    "    plt.bar(x + width/2, forgotten_probs, width, label='Forgotten', alpha=0.7)\n",
    "    for fd in forgotten_digits:\n",
    "        if fd in digits:\n",
    "            plt.axvline(x=digits.index(fd), color='red', linestyle='--')\n",
    "    plt.xlabel('Digit')\n",
    "    plt.ylabel('Target Probability')\n",
    "    plt.title('Target Probability Comparison')\n",
    "    plt.xticks(x, digits)\n",
    "    plt.legend([f'Forgotten Digits: {forgotten_digits}'])\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    entropy_diff = [f - o for o, f in zip(original_entropies, forgotten_entropies)]\n",
    "    colors = ['red' if d in forgotten_digits else 'blue' for d in digits]\n",
    "    plt.bar(x, entropy_diff, color=colors, alpha=0.7)\n",
    "    plt.xlabel('Digit')\n",
    "    plt.ylabel('Entropy Difference (Forgotten - Original)')\n",
    "    plt.title('Entropy Difference')\n",
    "    plt.xticks(x, digits)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    prob_diff = [o - f for o, f in zip(original_probs, forgotten_probs)]\n",
    "    colors = ['red' if d in forgotten_digits else 'blue' for d in digits]\n",
    "    plt.bar(x, prob_diff, color=colors, alpha=0.7)\n",
    "    plt.xlabel('Digit')\n",
    "    plt.ylabel('Probability Difference (Original - Forgotten)')\n",
    "    plt.title('Probability Difference')\n",
    "    plt.xticks(x, digits)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.exp_root_dir, 'comparison_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def display_sample_grids(config):\n",
    "    fig, axes = plt.subplots(2, 10, figsize=(20, 4))\n",
    "    for digit in range(10):\n",
    "        orig_grid_path = os.path.join(config.sample_dir, f'original_digit_{digit}_grid.png')\n",
    "        if os.path.exists(orig_grid_path):\n",
    "            orig_img = Image.open(orig_grid_path)\n",
    "            axes[0, digit].imshow(orig_img)\n",
    "            axes[0, digit].set_title(f'Original {digit}')\n",
    "            axes[0, digit].axis('off')\n",
    "\n",
    "        forg_grid_path = os.path.join(config.sample_dir, f'forgotten_digit_{digit}_grid.png')\n",
    "        if os.path.exists(forg_grid_path):\n",
    "            forg_img = Image.open(forg_grid_path)\n",
    "            axes[1, digit].imshow(forg_img)\n",
    "            axes[1, digit].set_title(f'Forgotten {digit}')\n",
    "            axes[1, digit].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.exp_root_dir, 'sample_comparison_grid.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def main(i=[0]):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    config = Config(i)\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"Digit to forget: {config.digit_to_forget}\")\n",
    "    print(f\"Lambda (EWC): {config.lmbda}\")\n",
    "    print(f\"Gamma: {config.gamma}\")\n",
    "    print(f\"Forgetting iterations: {config.n_forget_iters}\")\n",
    "    print(f\"Load existing models: {config.load_existing_models}\")\n",
    "    print(f\"Output directory: {config.exp_root_dir}\")\n",
    "\n",
    "    if config.load_existing_models:\n",
    "        original_vae = load_model(OneHotCVAE, 'original_vae.pt', config, device)\n",
    "    else:\n",
    "        original_vae = None\n",
    "\n",
    "    if original_vae is None:\n",
    "        original_vae = train_cvae(config, device)\n",
    "    else:\n",
    "        print(\"Using pre-trained original VAE\")\n",
    "\n",
    "    if config.load_existing_models:\n",
    "        fisher_dict = load_fisher_matrix(config)\n",
    "    else:\n",
    "        fisher_dict = None\n",
    "\n",
    "    if fisher_dict is None:\n",
    "        fisher_dict = calculate_fim(original_vae, config, device)\n",
    "    else:\n",
    "        print(\"Using pre-calculated Fisher matrix\")\n",
    "\n",
    "    generate_samples(original_vae, config, device, \"original\")\n",
    "\n",
    "    forgotten_model_name = f'forgotten_vae_digit_{config.digit_to_forget}.pt'\n",
    "    if config.load_existing_models:\n",
    "        forgotten_vae = load_model(OneHotCVAE, forgotten_model_name, config, device)\n",
    "    else:\n",
    "        forgotten_vae = None\n",
    "\n",
    "    if forgotten_vae is None:\n",
    "        forgotten_vae, losses = train_forgetting(original_vae, fisher_dict, config, device)\n",
    "    else:\n",
    "        print(f\"Using pre-trained forgetting model for digit {config.digit_to_forget}\")\n",
    "        losses = []\n",
    "\n",
    "    generate_samples(forgotten_vae, config, device, \"forgotten\")\n",
    "\n",
    "    if config.load_existing_models:\n",
    "        classifier = load_classifier(config, device)\n",
    "    else:\n",
    "        classifier = None\n",
    "\n",
    "    if classifier is None:\n",
    "        classifier = train_classifier(config, device)\n",
    "    else:\n",
    "        print(\"Using pre-trained classifier\")\n",
    "\n",
    "    results = evaluate_with_classifier(classifier, config.sample_dir, device)\n",
    "    create_comparison_plots(results, config)\n",
    "    display_sample_grids(config)\n",
    "    print(\"\\nRESULTS SUMMARY\")\n",
    "    forgotten_digit = config.digit_to_forget\n",
    "    orig_key = f'original_digit_{forgotten_digit}'\n",
    "    forg_key = f'forgotten_digit_{forgotten_digit}'\n",
    "\n",
    "    if orig_key in results and forg_key in results:\n",
    "        print(f\"\\nForgotten Digit {forgotten_digit}:\")\n",
    "        print(f\"  Original - Entropy: {results[orig_key]['entropy']:.4f}, Target Prob: {results[orig_key]['target_prob']:.4f}\")\n",
    "        print(f\"  Forgotten - Entropy: {results[forg_key]['entropy']:.4f}, Target Prob: {results[forg_key]['target_prob']:.4f}\")\n",
    "        print(f\"  Entropy Change: {results[forg_key]['entropy'] - results[orig_key]['entropy']:.4f}\")\n",
    "        print(f\"  Probability Change: {results[orig_key]['target_prob'] - results[forg_key]['target_prob']:.4f}\")\n",
    "\n",
    "    print(f\"\\nAll results and visualizations saved to: {config.exp_root_dir}\")\n",
    "    print(f\"All models saved to: {config.base_model_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for i in range (10):\n",
    "        main(i)     # forget each digit\n",
    "    main([3, 7])    # example of multiple forgeting"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1350.846878,
   "end_time": "2025-10-22T23:33:25.922270",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-22T23:10:55.075392",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
