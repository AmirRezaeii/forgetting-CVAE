{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da363f4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-23T11:03:27.683545Z",
     "iopub.status.busy": "2025-10-23T11:03:27.683349Z",
     "iopub.status.idle": "2025-10-23T12:17:05.443145Z",
     "shell.execute_reply": "2025-10-23T12:17:05.442231Z"
    },
    "id": "qr_prdZvgcUp",
    "outputId": "3f03cea2-9e30-4796-c228-9a8ce98fa51a",
    "papermill": {
     "duration": 4417.764592,
     "end_time": "2025-10-23T12:17:05.444550",
     "exception": false,
     "start_time": "2025-10-23T11:03:27.679958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Configuration:\n",
      "Digit to forget: [0]\n",
      "Lambda (EWC): 100\n",
      "Gamma: 1\n",
      "Forgetting iterations: 10000\n",
      "Load existing models: True\n",
      "Output directory: ./results/forgetting_test_0\n",
      "No saved model found at: ./saved_models/original_vae.pt\n",
      "Training original CVAE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 11.6MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 343kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.21MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 11.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 70280.694299\n",
      "Epoch 10, Loss: 32261.237616\n",
      "Epoch 20, Loss: 29536.205656\n",
      "Epoch 30, Loss: 28330.566390\n",
      "Epoch 40, Loss: 27620.328358\n",
      "Epoch 50, Loss: 27135.540372\n",
      "Epoch 60, Loss: 26766.353183\n",
      "Epoch 70, Loss: 26464.342640\n",
      "Epoch 80, Loss: 26212.664358\n",
      "Epoch 90, Loss: 26001.161403\n",
      "Epoch 100, Loss: 25827.016464\n",
      "Epoch 110, Loss: 25663.552078\n",
      "Epoch 120, Loss: 25523.917283\n",
      "Epoch 130, Loss: 25393.332484\n",
      "Epoch 140, Loss: 25285.402148\n",
      "Epoch 150, Loss: 25181.184275\n",
      "Epoch 160, Loss: 25083.969993\n",
      "Epoch 170, Loss: 25002.450278\n",
      "Epoch 180, Loss: 24928.507468\n",
      "Epoch 190, Loss: 24852.385273\n",
      "Epoch 200, Loss: 24780.282251\n",
      "Epoch 210, Loss: 24723.579692\n",
      "Epoch 220, Loss: 24662.064312\n",
      "Epoch 230, Loss: 24612.035302\n",
      "Epoch 240, Loss: 24558.689582\n",
      "Epoch 250, Loss: 24510.194303\n",
      "Epoch 260, Loss: 24461.383818\n",
      "Epoch 270, Loss: 24418.345055\n",
      "Epoch 280, Loss: 24371.031437\n",
      "Epoch 290, Loss: 24340.946119\n",
      "Epoch 300, Loss: 24297.497943\n",
      "Epoch 310, Loss: 24263.469552\n",
      "Epoch 320, Loss: 24227.682397\n",
      "Epoch 330, Loss: 24200.135771\n",
      "Epoch 340, Loss: 24163.750062\n",
      "Epoch 350, Loss: 24134.015791\n",
      "Epoch 360, Loss: 24105.218143\n",
      "Epoch 370, Loss: 24076.038389\n",
      "Epoch 380, Loss: 24045.345657\n",
      "Epoch 390, Loss: 24021.297037\n",
      "Epoch 400, Loss: 23991.140733\n",
      "Epoch 410, Loss: 23971.466548\n",
      "Epoch 420, Loss: 23941.740920\n",
      "Epoch 430, Loss: 23928.557875\n",
      "Epoch 440, Loss: 23909.452028\n",
      "Epoch 450, Loss: 23881.871883\n",
      "Epoch 460, Loss: 23858.626712\n",
      "Epoch 470, Loss: 23842.589466\n",
      "Epoch 480, Loss: 23818.708319\n",
      "Epoch 490, Loss: 23800.907871\n",
      "Model saved to: ./saved_models/original_vae.pt\n",
      "No saved Fisher matrix found at: ./saved_models/fisher_matrix.pkl\n",
      "Calculating FIM\n",
      "Fisher matrix saved to: ./saved_models/fisher_matrix.pkl\n",
      "Generating original samples\n",
      "No saved model found at: ./saved_models/forgotten_vae_digit_[0].pt\n",
      "Training to forget [0]\n",
      "Forgetting Step: 1000, Loss: 170552.156250\n",
      "Forgetting Step: 2000, Loss: 169779.468750\n",
      "Forgetting Step: 3000, Loss: 168222.937500\n",
      "Forgetting Step: 4000, Loss: 167900.796875\n",
      "Forgetting Step: 5000, Loss: 167224.562500\n",
      "Forgetting Step: 6000, Loss: 166814.500000\n",
      "Forgetting Step: 7000, Loss: 166856.250000\n",
      "Forgetting Step: 8000, Loss: 166416.937500\n",
      "Forgetting Step: 9000, Loss: 166551.171875\n",
      "Forgetting Step: 10000, Loss: 166147.640625\n",
      "Model saved to: ./saved_models/forgotten_vae_digits_0.pt\n",
      "Generating forgotten samples\n",
      "No saved classifier found at: ./saved_models/classifier.pt\n",
      "Training classifier\n",
      "Classifier saved to: ./saved_models/classifier.pt\n",
      "Evaluating samples with classifier\n",
      "\n",
      "RESULTS SUMMARY\n",
      "\n",
      "All results and visualizations saved to: ./results/forgetting_test_0\n",
      "All models saved to: ./saved_models\n",
      "Using device: cuda\n",
      "Configuration:\n",
      "Digit to forget: [1]\n",
      "Lambda (EWC): 100\n",
      "Gamma: 1\n",
      "Forgetting iterations: 10000\n",
      "Load existing models: True\n",
      "Output directory: ./results/forgetting_test_1\n",
      "Loading model from: ./saved_models/original_vae.pt\n",
      "Using pre-trained original VAE\n",
      "Loading Fisher matrix from: ./saved_models/fisher_matrix.pkl\n",
      "Using pre-calculated Fisher matrix\n",
      "Generating original samples\n",
      "No saved model found at: ./saved_models/forgotten_vae_digit_[1].pt\n",
      "Training to forget [1]\n",
      "Forgetting Step: 1000, Loss: 172527.921875\n",
      "Forgetting Step: 2000, Loss: 170558.953125\n",
      "Forgetting Step: 3000, Loss: 170177.375000\n",
      "Forgetting Step: 4000, Loss: 169122.218750\n",
      "Forgetting Step: 5000, Loss: 168546.390625\n",
      "Forgetting Step: 6000, Loss: 168302.000000\n",
      "Forgetting Step: 7000, Loss: 168001.359375\n",
      "Forgetting Step: 8000, Loss: 168192.093750\n",
      "Forgetting Step: 9000, Loss: 167873.750000\n",
      "Forgetting Step: 10000, Loss: 168078.781250\n",
      "Model saved to: ./saved_models/forgotten_vae_digits_1.pt\n",
      "Generating forgotten samples\n",
      "Loading classifier from: ./saved_models/classifier.pt\n",
      "Using pre-trained classifier\n",
      "Evaluating samples with classifier\n",
      "\n",
      "RESULTS SUMMARY\n",
      "\n",
      "All results and visualizations saved to: ./results/forgetting_test_1\n",
      "All models saved to: ./saved_models\n",
      "Using device: cuda\n",
      "Configuration:\n",
      "Digit to forget: [2]\n",
      "Lambda (EWC): 100\n",
      "Gamma: 1\n",
      "Forgetting iterations: 10000\n",
      "Load existing models: True\n",
      "Output directory: ./results/forgetting_test_2\n",
      "Loading model from: ./saved_models/original_vae.pt\n",
      "Using pre-trained original VAE\n",
      "Loading Fisher matrix from: ./saved_models/fisher_matrix.pkl\n",
      "Using pre-calculated Fisher matrix\n",
      "Generating original samples\n",
      "No saved model found at: ./saved_models/forgotten_vae_digit_[2].pt\n",
      "Training to forget [2]\n",
      "Forgetting Step: 1000, Loss: 171489.546875\n",
      "Forgetting Step: 2000, Loss: 169598.875000\n",
      "Forgetting Step: 3000, Loss: 168221.812500\n",
      "Forgetting Step: 4000, Loss: 168555.859375\n",
      "Forgetting Step: 5000, Loss: 167307.953125\n",
      "Forgetting Step: 6000, Loss: 166613.406250\n",
      "Forgetting Step: 7000, Loss: 166526.250000\n",
      "Forgetting Step: 8000, Loss: 166777.531250\n",
      "Forgetting Step: 9000, Loss: 166791.093750\n",
      "Forgetting Step: 10000, Loss: 166406.593750\n",
      "Model saved to: ./saved_models/forgotten_vae_digits_2.pt\n",
      "Generating forgotten samples\n",
      "Loading classifier from: ./saved_models/classifier.pt\n",
      "Using pre-trained classifier\n",
      "Evaluating samples with classifier\n",
      "\n",
      "RESULTS SUMMARY\n",
      "\n",
      "All results and visualizations saved to: ./results/forgetting_test_2\n",
      "All models saved to: ./saved_models\n",
      "Using device: cuda\n",
      "Configuration:\n",
      "Digit to forget: [3]\n",
      "Lambda (EWC): 100\n",
      "Gamma: 1\n",
      "Forgetting iterations: 10000\n",
      "Load existing models: True\n",
      "Output directory: ./results/forgetting_test_3\n",
      "Loading model from: ./saved_models/original_vae.pt\n",
      "Using pre-trained original VAE\n",
      "Loading Fisher matrix from: ./saved_models/fisher_matrix.pkl\n",
      "Using pre-calculated Fisher matrix\n",
      "Generating original samples\n",
      "No saved model found at: ./saved_models/forgotten_vae_digit_[3].pt\n",
      "Training to forget [3]\n",
      "Forgetting Step: 1000, Loss: 171517.296875\n",
      "Forgetting Step: 2000, Loss: 170187.859375\n",
      "Forgetting Step: 3000, Loss: 169422.796875\n",
      "Forgetting Step: 4000, Loss: 166963.546875\n",
      "Forgetting Step: 5000, Loss: 167925.046875\n",
      "Forgetting Step: 6000, Loss: 167706.421875\n",
      "Forgetting Step: 7000, Loss: 167280.734375\n",
      "Forgetting Step: 8000, Loss: 167692.515625\n",
      "Forgetting Step: 9000, Loss: 167213.093750\n",
      "Forgetting Step: 10000, Loss: 166567.515625\n",
      "Model saved to: ./saved_models/forgotten_vae_digits_3.pt\n",
      "Generating forgotten samples\n",
      "Loading classifier from: ./saved_models/classifier.pt\n",
      "Using pre-trained classifier\n",
      "Evaluating samples with classifier\n",
      "\n",
      "RESULTS SUMMARY\n",
      "\n",
      "All results and visualizations saved to: ./results/forgetting_test_3\n",
      "All models saved to: ./saved_models\n",
      "Using device: cuda\n",
      "Configuration:\n",
      "Digit to forget: [4]\n",
      "Lambda (EWC): 100\n",
      "Gamma: 1\n",
      "Forgetting iterations: 10000\n",
      "Load existing models: True\n",
      "Output directory: ./results/forgetting_test_4\n",
      "Loading model from: ./saved_models/original_vae.pt\n",
      "Using pre-trained original VAE\n",
      "Loading Fisher matrix from: ./saved_models/fisher_matrix.pkl\n",
      "Using pre-calculated Fisher matrix\n",
      "Generating original samples\n",
      "No saved model found at: ./saved_models/forgotten_vae_digit_[4].pt\n",
      "Training to forget [4]\n",
      "Forgetting Step: 1000, Loss: 170980.687500\n",
      "Forgetting Step: 2000, Loss: 169910.031250\n",
      "Forgetting Step: 3000, Loss: 170061.093750\n",
      "Forgetting Step: 4000, Loss: 167859.203125\n",
      "Forgetting Step: 5000, Loss: 168044.703125\n",
      "Forgetting Step: 6000, Loss: 167564.828125\n",
      "Forgetting Step: 7000, Loss: 166214.031250\n",
      "Forgetting Step: 8000, Loss: 166670.375000\n",
      "Forgetting Step: 9000, Loss: 167007.140625\n",
      "Forgetting Step: 10000, Loss: 166770.218750\n",
      "Model saved to: ./saved_models/forgotten_vae_digits_4.pt\n",
      "Generating forgotten samples\n",
      "Loading classifier from: ./saved_models/classifier.pt\n",
      "Using pre-trained classifier\n",
      "Evaluating samples with classifier\n",
      "\n",
      "RESULTS SUMMARY\n",
      "\n",
      "All results and visualizations saved to: ./results/forgetting_test_4\n",
      "All models saved to: ./saved_models\n",
      "Using device: cuda\n",
      "Configuration:\n",
      "Digit to forget: [5]\n",
      "Lambda (EWC): 100\n",
      "Gamma: 1\n",
      "Forgetting iterations: 10000\n",
      "Load existing models: True\n",
      "Output directory: ./results/forgetting_test_5\n",
      "Loading model from: ./saved_models/original_vae.pt\n",
      "Using pre-trained original VAE\n",
      "Loading Fisher matrix from: ./saved_models/fisher_matrix.pkl\n",
      "Using pre-calculated Fisher matrix\n",
      "Generating original samples\n",
      "No saved model found at: ./saved_models/forgotten_vae_digit_[5].pt\n",
      "Training to forget [5]\n",
      "Forgetting Step: 1000, Loss: 170645.890625\n",
      "Forgetting Step: 2000, Loss: 168403.703125\n",
      "Forgetting Step: 3000, Loss: 168026.265625\n",
      "Forgetting Step: 4000, Loss: 166998.750000\n",
      "Forgetting Step: 5000, Loss: 167244.468750\n",
      "Forgetting Step: 6000, Loss: 167853.937500\n",
      "Forgetting Step: 7000, Loss: 167025.875000\n",
      "Forgetting Step: 8000, Loss: 166528.093750\n",
      "Forgetting Step: 9000, Loss: 166471.437500\n",
      "Forgetting Step: 10000, Loss: 165765.843750\n",
      "Model saved to: ./saved_models/forgotten_vae_digits_5.pt\n",
      "Generating forgotten samples\n",
      "Loading classifier from: ./saved_models/classifier.pt\n",
      "Using pre-trained classifier\n",
      "Evaluating samples with classifier\n",
      "\n",
      "RESULTS SUMMARY\n",
      "\n",
      "All results and visualizations saved to: ./results/forgetting_test_5\n",
      "All models saved to: ./saved_models\n",
      "Using device: cuda\n",
      "Configuration:\n",
      "Digit to forget: [6]\n",
      "Lambda (EWC): 100\n",
      "Gamma: 1\n",
      "Forgetting iterations: 10000\n",
      "Load existing models: True\n",
      "Output directory: ./results/forgetting_test_6\n",
      "Loading model from: ./saved_models/original_vae.pt\n",
      "Using pre-trained original VAE\n",
      "Loading Fisher matrix from: ./saved_models/fisher_matrix.pkl\n",
      "Using pre-calculated Fisher matrix\n",
      "Generating original samples\n",
      "No saved model found at: ./saved_models/forgotten_vae_digit_[6].pt\n",
      "Training to forget [6]\n",
      "Forgetting Step: 1000, Loss: 171073.109375\n",
      "Forgetting Step: 2000, Loss: 169738.828125\n",
      "Forgetting Step: 3000, Loss: 168987.718750\n",
      "Forgetting Step: 4000, Loss: 168067.687500\n",
      "Forgetting Step: 5000, Loss: 167597.609375\n",
      "Forgetting Step: 6000, Loss: 167801.093750\n",
      "Forgetting Step: 7000, Loss: 167361.093750\n",
      "Forgetting Step: 8000, Loss: 167014.843750\n",
      "Forgetting Step: 9000, Loss: 167070.437500\n",
      "Forgetting Step: 10000, Loss: 166214.078125\n",
      "Model saved to: ./saved_models/forgotten_vae_digits_6.pt\n",
      "Generating forgotten samples\n",
      "Loading classifier from: ./saved_models/classifier.pt\n",
      "Using pre-trained classifier\n",
      "Evaluating samples with classifier\n",
      "\n",
      "RESULTS SUMMARY\n",
      "\n",
      "All results and visualizations saved to: ./results/forgetting_test_6\n",
      "All models saved to: ./saved_models\n",
      "Using device: cuda\n",
      "Configuration:\n",
      "Digit to forget: [7]\n",
      "Lambda (EWC): 100\n",
      "Gamma: 1\n",
      "Forgetting iterations: 10000\n",
      "Load existing models: True\n",
      "Output directory: ./results/forgetting_test_7\n",
      "Loading model from: ./saved_models/original_vae.pt\n",
      "Using pre-trained original VAE\n",
      "Loading Fisher matrix from: ./saved_models/fisher_matrix.pkl\n",
      "Using pre-calculated Fisher matrix\n",
      "Generating original samples\n",
      "No saved model found at: ./saved_models/forgotten_vae_digit_[7].pt\n",
      "Training to forget [7]\n",
      "Forgetting Step: 1000, Loss: 171937.171875\n",
      "Forgetting Step: 2000, Loss: 170607.109375\n",
      "Forgetting Step: 3000, Loss: 169651.328125\n",
      "Forgetting Step: 4000, Loss: 168792.890625\n",
      "Forgetting Step: 5000, Loss: 168388.171875\n",
      "Forgetting Step: 6000, Loss: 168319.125000\n",
      "Forgetting Step: 7000, Loss: 168070.343750\n",
      "Forgetting Step: 8000, Loss: 167373.218750\n",
      "Forgetting Step: 9000, Loss: 168718.593750\n",
      "Forgetting Step: 10000, Loss: 166656.234375\n",
      "Model saved to: ./saved_models/forgotten_vae_digits_7.pt\n",
      "Generating forgotten samples\n",
      "Loading classifier from: ./saved_models/classifier.pt\n",
      "Using pre-trained classifier\n",
      "Evaluating samples with classifier\n",
      "\n",
      "RESULTS SUMMARY\n",
      "\n",
      "All results and visualizations saved to: ./results/forgetting_test_7\n",
      "All models saved to: ./saved_models\n",
      "Using device: cuda\n",
      "Configuration:\n",
      "Digit to forget: [8]\n",
      "Lambda (EWC): 100\n",
      "Gamma: 1\n",
      "Forgetting iterations: 10000\n",
      "Load existing models: True\n",
      "Output directory: ./results/forgetting_test_8\n",
      "Loading model from: ./saved_models/original_vae.pt\n",
      "Using pre-trained original VAE\n",
      "Loading Fisher matrix from: ./saved_models/fisher_matrix.pkl\n",
      "Using pre-calculated Fisher matrix\n",
      "Generating original samples\n",
      "No saved model found at: ./saved_models/forgotten_vae_digit_[8].pt\n",
      "Training to forget [8]\n",
      "Forgetting Step: 1000, Loss: 168952.296875\n",
      "Forgetting Step: 2000, Loss: 168905.265625\n",
      "Forgetting Step: 3000, Loss: 167600.890625\n",
      "Forgetting Step: 4000, Loss: 166945.312500\n",
      "Forgetting Step: 5000, Loss: 166967.265625\n",
      "Forgetting Step: 6000, Loss: 166476.437500\n",
      "Forgetting Step: 7000, Loss: 167431.984375\n",
      "Forgetting Step: 8000, Loss: 166811.625000\n",
      "Forgetting Step: 9000, Loss: 166664.859375\n",
      "Forgetting Step: 10000, Loss: 165826.328125\n",
      "Model saved to: ./saved_models/forgotten_vae_digits_8.pt\n",
      "Generating forgotten samples\n",
      "Loading classifier from: ./saved_models/classifier.pt\n",
      "Using pre-trained classifier\n",
      "Evaluating samples with classifier\n",
      "\n",
      "RESULTS SUMMARY\n",
      "\n",
      "All results and visualizations saved to: ./results/forgetting_test_8\n",
      "All models saved to: ./saved_models\n",
      "Using device: cuda\n",
      "Configuration:\n",
      "Digit to forget: [9]\n",
      "Lambda (EWC): 100\n",
      "Gamma: 1\n",
      "Forgetting iterations: 10000\n",
      "Load existing models: True\n",
      "Output directory: ./results/forgetting_test_9\n",
      "Loading model from: ./saved_models/original_vae.pt\n",
      "Using pre-trained original VAE\n",
      "Loading Fisher matrix from: ./saved_models/fisher_matrix.pkl\n",
      "Using pre-calculated Fisher matrix\n",
      "Generating original samples\n",
      "No saved model found at: ./saved_models/forgotten_vae_digit_[9].pt\n",
      "Training to forget [9]\n",
      "Forgetting Step: 1000, Loss: 172007.265625\n",
      "Forgetting Step: 2000, Loss: 169952.921875\n",
      "Forgetting Step: 3000, Loss: 169029.078125\n",
      "Forgetting Step: 4000, Loss: 168922.421875\n",
      "Forgetting Step: 5000, Loss: 168249.718750\n",
      "Forgetting Step: 6000, Loss: 168972.218750\n",
      "Forgetting Step: 7000, Loss: 168026.406250\n",
      "Forgetting Step: 8000, Loss: 167420.687500\n",
      "Forgetting Step: 9000, Loss: 167737.218750\n",
      "Forgetting Step: 10000, Loss: 166965.218750\n",
      "Model saved to: ./saved_models/forgotten_vae_digits_9.pt\n",
      "Generating forgotten samples\n",
      "Loading classifier from: ./saved_models/classifier.pt\n",
      "Using pre-trained classifier\n",
      "Evaluating samples with classifier\n",
      "\n",
      "RESULTS SUMMARY\n",
      "\n",
      "All results and visualizations saved to: ./results/forgetting_test_9\n",
      "All models saved to: ./saved_models\n",
      "Using device: cuda\n",
      "Configuration:\n",
      "Digit to forget: [3, 7]\n",
      "Lambda (EWC): 100\n",
      "Gamma: 1\n",
      "Forgetting iterations: 10000\n",
      "Load existing models: True\n",
      "Output directory: ./results/forgetting_test_3_7\n",
      "Loading model from: ./saved_models/original_vae.pt\n",
      "Using pre-trained original VAE\n",
      "Loading Fisher matrix from: ./saved_models/fisher_matrix.pkl\n",
      "Using pre-calculated Fisher matrix\n",
      "Generating original samples\n",
      "No saved model found at: ./saved_models/forgotten_vae_digit_[3, 7].pt\n",
      "Training to forget [3, 7]\n",
      "Forgetting Step: 1000, Loss: 172518.296875\n",
      "Forgetting Step: 2000, Loss: 170190.140625\n",
      "Forgetting Step: 3000, Loss: 170019.531250\n",
      "Forgetting Step: 4000, Loss: 168737.062500\n",
      "Forgetting Step: 5000, Loss: 169075.593750\n",
      "Forgetting Step: 6000, Loss: 167469.921875\n",
      "Forgetting Step: 7000, Loss: 168635.828125\n",
      "Forgetting Step: 8000, Loss: 167428.421875\n",
      "Forgetting Step: 9000, Loss: 167527.953125\n",
      "Forgetting Step: 10000, Loss: 167339.687500\n",
      "Model saved to: ./saved_models/forgotten_vae_digits_3_7.pt\n",
      "Generating forgotten samples\n",
      "Loading classifier from: ./saved_models/classifier.pt\n",
      "Using pre-trained classifier\n",
      "Evaluating samples with classifier\n",
      "\n",
      "RESULTS SUMMARY\n",
      "\n",
      "All results and visualizations saved to: ./results/forgetting_test_3_7\n",
      "All models saved to: ./saved_models\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import logging\n",
    "import copy\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "\n",
    "class OneHotCVAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim, class_size=10):\n",
    "        super(OneHotCVAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(x_dim + class_size, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc4 = nn.Linear(z_dim + class_size, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "\n",
    "    def encoder(self, x, c):\n",
    "        inputs = torch.cat([x,c], dim=1)\n",
    "        h = F.relu(self.fc1(inputs))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h)\n",
    "\n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def decoder(self, z, c):\n",
    "        inputs = torch.cat([z,c], dim=1)\n",
    "        h = F.relu(self.fc4(inputs))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return torch.sigmoid(self.fc6(h))\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, log_var = self.encoder(x.view(-1, 784), c)\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z, c), mu, log_var\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.bn1 = nn.BatchNorm2d(10)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.bn2 = nn.BatchNorm2d(20)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.bn1(self.conv1(x)), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.bn2(self.conv2(x))), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, forget=[0]):\n",
    "        self.dataset = \"MNIST\"\n",
    "        self.x_dim = 784\n",
    "        self.h_dim1 = 512\n",
    "        self.h_dim2 = 256\n",
    "        self.z_dim = 8\n",
    "        if isinstance(forget, int):\n",
    "            forget = [forget]\n",
    "        self.digit_to_forget = forget\n",
    "        self.lmbda = 100\n",
    "        self.gamma = 1\n",
    "        self.n_forget_iters = 10000\n",
    "        self.batch_size = 256\n",
    "        self.lr = 1e-4\n",
    "        self.n_samples_per_class = 10\n",
    "        self.load_existing_models = True\n",
    "        self.base_model_dir = \"./saved_models\"\n",
    "        forget_label = \"_\".join(map(str, forget)) if isinstance(forget, (list, tuple)) else str(forget)\n",
    "        self.exp_root_dir = f\"./results/forgetting_test_{forget_label}\"\n",
    "        self.log_dir = os.path.join(self.exp_root_dir, 'logs')\n",
    "        self.ckpt_dir = os.path.join(self.exp_root_dir, 'ckpts')\n",
    "        self.sample_dir = os.path.join(self.exp_root_dir, 'samples')\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        os.makedirs(self.ckpt_dir, exist_ok=True)\n",
    "        os.makedirs(self.sample_dir, exist_ok=True)\n",
    "        os.makedirs(self.base_model_dir, exist_ok=True)\n",
    "\n",
    "def save_model(model, filename, config=None):\n",
    "    save_path = os.path.join(config.base_model_dir, filename)\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': config\n",
    "    }, save_path)\n",
    "    print(f\"Model saved to: {save_path}\")\n",
    "\n",
    "def load_model(model_class, filename, config, device):\n",
    "    load_path = os.path.join(config.base_model_dir, filename)\n",
    "    if os.path.exists(load_path):\n",
    "        print(f\"Loading model from: {load_path}\")\n",
    "        try:\n",
    "            checkpoint = torch.load(load_path, map_location=device, weights_only=True)\n",
    "        except:\n",
    "            checkpoint = torch.load(load_path, map_location=device, weights_only=False)\n",
    "        model = model_class(config.x_dim, config.h_dim1, config.h_dim2, config.z_dim).to(device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        return model\n",
    "    else:\n",
    "        print(f\"No saved model found at: {load_path}\")\n",
    "        return None\n",
    "\n",
    "def save_fisher_matrix(fisher_dict, config):\n",
    "    fisher_path = os.path.join(config.base_model_dir, 'fisher_matrix.pkl')\n",
    "    with open(fisher_path, 'wb') as f:\n",
    "        pickle.dump(fisher_dict, f)\n",
    "    print(f\"Fisher matrix saved to: {fisher_path}\")\n",
    "\n",
    "def load_fisher_matrix(config):\n",
    "    fisher_path = os.path.join(config.base_model_dir, 'fisher_matrix.pkl')\n",
    "    if os.path.exists(fisher_path):\n",
    "        print(f\"Loading Fisher matrix from: {fisher_path}\")\n",
    "        with open(fisher_path, 'rb') as f:\n",
    "            fisher_dict = pickle.load(f)\n",
    "        return fisher_dict\n",
    "    else:\n",
    "        print(f\"No saved Fisher matrix found at: {fisher_path}\")\n",
    "        return None\n",
    "\n",
    "def save_classifier(classifier, config):\n",
    "    classifier_path = os.path.join(config.base_model_dir, 'classifier.pt')\n",
    "    torch.save(classifier.state_dict(), classifier_path)\n",
    "    print(f\"Classifier saved to: {classifier_path}\")\n",
    "\n",
    "def load_classifier(config, device):\n",
    "    classifier_path = os.path.join(config.base_model_dir, 'classifier.pt')\n",
    "    if os.path.exists(classifier_path):\n",
    "        print(f\"Loading classifier from: {classifier_path}\")\n",
    "        classifier = Classifier().to(device)\n",
    "        classifier.load_state_dict(torch.load(classifier_path, map_location=device, weights_only=True))\n",
    "        return classifier\n",
    "    else:\n",
    "        print(f\"No saved classifier found at: {classifier_path}\")\n",
    "        return None\n",
    "\n",
    "def train_cvae(config, device):\n",
    "    print(\"Training original CVAE\")\n",
    "    train_dataset = datasets.MNIST('./dataset', train=True, download=True,\n",
    "                                   transform=transforms.ToTensor())\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    vae = OneHotCVAE(config.x_dim, config.h_dim1, config.h_dim2, config.z_dim).to(device)\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=config.lr)\n",
    "    vae.train()\n",
    "    for epoch in range(500):\n",
    "        train_loss = 0\n",
    "        for batch_idx, (data, labels) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            c = F.one_hot(labels, 10).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            recon_batch, mu, log_var = vae(data, c)\n",
    "            loss = loss_function(recon_batch, data, mu, log_var)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}, Loss: {train_loss/len(train_loader):.6f}')\n",
    "    save_model(vae, 'original_vae.pt', config)\n",
    "    return vae\n",
    "\n",
    "def calculate_fim(vae, config, device):\n",
    "    print(\"Calculating FIM\")\n",
    "    fisher_dict = {}\n",
    "    for name, param in vae.named_parameters():\n",
    "        fisher_dict[name] = torch.zeros_like(param.data)\n",
    "    n_fim_samples = 50000\n",
    "    for _ in range(n_fim_samples):\n",
    "        with torch.no_grad():\n",
    "            z = torch.randn(1, config.z_dim).to(device)\n",
    "            c = torch.randint(0, 10, (1,)).to(device)\n",
    "            c = F.one_hot(c, 10)\n",
    "            sample = vae.decoder(z, c)\n",
    "        vae.zero_grad()\n",
    "        recon_batch, mu, log_var = vae(sample, c)\n",
    "        loss = loss_function(recon_batch, sample, mu, log_var)\n",
    "        loss.backward()\n",
    "        for name, param in vae.named_parameters():\n",
    "            fisher_dict[name] += (param.grad.data ** 2) / n_fim_samples\n",
    "    save_fisher_matrix(fisher_dict, config)\n",
    "    return fisher_dict\n",
    "\n",
    "def train_forgetting(original_vae, fisher_dict, config, device):\n",
    "    print(f\"Training to forget {config.digit_to_forget}\")\n",
    "    vae = copy.deepcopy(original_vae)\n",
    "    vae.train()\n",
    "    params_mle_dict = {}\n",
    "    for name, param in original_vae.named_parameters():\n",
    "        params_mle_dict[name] = param.data.clone()\n",
    "    optimizer = optim.Adam(vae.parameters(), lr=config.lr)\n",
    "    label_choices = [d for d in range(10) if d not in config.digit_to_forget]\n",
    "    vae_clone = copy.deepcopy(vae)\n",
    "    vae_clone.eval()\n",
    "    losses = []\n",
    "    for step in range(config.n_forget_iters):\n",
    "        c_remember = torch.from_numpy(np.random.choice(label_choices, size=config.batch_size)).to(device)\n",
    "        c_remember = F.one_hot(c_remember, 10)\n",
    "        z_remember = torch.randn((config.batch_size, config.z_dim)).to(device)\n",
    "        forget_labels = np.random.choice(config.digit_to_forget, size=config.batch_size)\n",
    "        c_forget = torch.from_numpy(forget_labels).to(device)\n",
    "        c_forget = F.one_hot(c_forget, 10)\n",
    "        out_forget = torch.rand((config.batch_size, 1, 28, 28)).to(device)\n",
    "        with torch.no_grad():\n",
    "            out_remember = vae_clone.decoder(z_remember, c_remember).view(-1, 1, 28, 28)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, log_var = vae(out_forget, c_forget)\n",
    "        loss = loss_function(recon_batch, out_forget, mu, log_var)\n",
    "        recon_batch, mu, log_var = vae(out_remember, c_remember)\n",
    "        loss += config.gamma * loss_function(recon_batch, out_remember, mu, log_var)\n",
    "        for n, p in vae.named_parameters():\n",
    "            _loss = fisher_dict[n].to(device) * (p - params_mle_dict[n].to(device)) ** 2\n",
    "            loss += config.lmbda * _loss.sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        if (step + 1) % 1000 == 0:\n",
    "            print(f'Forgetting Step: {step+1}, Loss: {loss.item():.6f}')\n",
    "    forgotten_model_name = f'forgotten_vae_digits_{\"_\".join(map(str, config.digit_to_forget))}.pt'\n",
    "    save_model(vae, forgotten_model_name, config)\n",
    "    return vae, losses\n",
    "\n",
    "def generate_samples(vae, config, device, prefix=\"original\"):\n",
    "    print(f\"Generating {prefix} samples\")\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        for digit in range(10):\n",
    "            z = torch.randn((config.n_samples_per_class, config.z_dim)).to(device)\n",
    "            c = (torch.ones(config.n_samples_per_class, dtype=int) * digit).to(device)\n",
    "            c = F.one_hot(c, 10)\n",
    "            samples = vae.decoder(z, c).view(-1, 1, 28, 28)\n",
    "            digit_dir = os.path.join(config.sample_dir, prefix, f'digit_{digit}')\n",
    "            os.makedirs(digit_dir, exist_ok=True)\n",
    "            for i, sample in enumerate(samples):\n",
    "                save_image(sample, os.path.join(digit_dir, f'sample_{i}.png'))\n",
    "            grid = make_grid(samples, nrow=5)\n",
    "            save_image(grid, os.path.join(config.sample_dir, f'{prefix}_digit_{digit}_grid.png'))\n",
    "\n",
    "\n",
    "def train_classifier(config, device):\n",
    "    print(\"Training classifier\")\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./dataset', train=True, download=True,\n",
    "                      transform=transforms.ToTensor()),\n",
    "        batch_size=64, shuffle=True)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./dataset', train=False, download=True,\n",
    "                      transform=transforms.ToTensor()),\n",
    "        batch_size=1000, shuffle=True)\n",
    "    classifier = Classifier().to(device)\n",
    "    optimizer = optim.Adam(classifier.parameters(), lr=1e-4)\n",
    "    classifier.train()\n",
    "    for epoch in range(20):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = classifier(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    save_classifier(classifier, config)\n",
    "    return classifier\n",
    "\n",
    "def evaluate_with_classifier(classifier, sample_dir, device):\n",
    "    print(\"Evaluating samples with classifier\")\n",
    "    results = {}\n",
    "    for digit in range(10):\n",
    "        original_samples = []\n",
    "        forgotten_samples = []\n",
    "        orig_dir = os.path.join(sample_dir, 'original', f'digit_{digit}')\n",
    "        forg_dir = os.path.join(sample_dir, 'forgotten', f'digit_{digit}')\n",
    "        if os.path.exists(orig_dir):\n",
    "            for img_file in os.listdir(orig_dir)[:10]:\n",
    "                img = Image.open(os.path.join(orig_dir, img_file)).convert('L')\n",
    "                img_tensor = transforms.ToTensor()(img).unsqueeze(0).to(device)\n",
    "                original_samples.append(img_tensor)\n",
    "        if os.path.exists(forg_dir):\n",
    "            for img_file in os.listdir(forg_dir)[:10]:\n",
    "                img = Image.open(os.path.join(forg_dir, img_file)).convert('L')\n",
    "                img_tensor = transforms.ToTensor()(img).unsqueeze(0).to(device)\n",
    "                forgotten_samples.append(img_tensor)\n",
    "\n",
    "        if original_samples:\n",
    "            orig_tensor = torch.cat(original_samples)\n",
    "            with torch.no_grad():\n",
    "                orig_log_probs = classifier(orig_tensor)\n",
    "                orig_probs = torch.exp(orig_log_probs)\n",
    "                orig_entropy = -torch.sum(orig_probs * orig_log_probs, dim=1).mean().item()\n",
    "                orig_target_prob = orig_probs[:, digit].mean().item()\n",
    "            results[f'original_digit_{digit}'] = {\n",
    "                'entropy': orig_entropy,\n",
    "                'target_prob': orig_target_prob\n",
    "            }\n",
    "\n",
    "        if forgotten_samples:\n",
    "            forg_tensor = torch.cat(forgotten_samples)\n",
    "            with torch.no_grad():\n",
    "                forg_log_probs = classifier(forg_tensor)\n",
    "                forg_probs = torch.exp(forg_log_probs)\n",
    "                forg_entropy = -torch.sum(forg_probs * forg_log_probs, dim=1).mean().item()\n",
    "                forg_target_prob = forg_probs[:, digit].mean().item()\n",
    "            results[f'forgotten_digit_{digit}'] = {\n",
    "                'entropy': forg_entropy,\n",
    "                'target_prob': forg_target_prob\n",
    "            }\n",
    "    return results\n",
    "\n",
    "def create_comparison_plots(results, config):\n",
    "    if hasattr(config, \"digits_to_forget\"):\n",
    "        forgotten_digits = config.digits_to_forget\n",
    "    else:\n",
    "        forgotten_digits = [config.digit_to_forget]\n",
    "\n",
    "    original_entropies = []\n",
    "    forgotten_entropies = []\n",
    "    digits = []\n",
    "    for digit in range(10):\n",
    "        orig_key = f'original_digit_{digit}'\n",
    "        forg_key = f'forgotten_digit_{digit}'\n",
    "        if orig_key in results and forg_key in results:\n",
    "            original_entropies.append(results[orig_key]['entropy'])\n",
    "            forgotten_entropies.append(results[forg_key]['entropy'])\n",
    "            digits.append(digit)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    x = np.arange(len(digits))\n",
    "    width = 0.35\n",
    "    plt.bar(x - width/2, original_entropies, width, label='Original', alpha=0.7)\n",
    "    plt.bar(x + width/2, forgotten_entropies, width, label='Forgotten', alpha=0.7)\n",
    "    for fd in forgotten_digits:\n",
    "        if fd in digits:\n",
    "            plt.axvline(x=digits.index(fd), color='red', linestyle='--')\n",
    "    plt.xlabel('Digit')\n",
    "    plt.ylabel('Entropy')\n",
    "    plt.title('Entropy Comparison')\n",
    "    plt.xticks(x, digits)\n",
    "    plt.legend([f'Forgotten Digits: {forgotten_digits}'])\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    original_probs = [results[f'original_digit_{d}']['target_prob'] for d in digits]\n",
    "    forgotten_probs = [results[f'forgotten_digit_{d}']['target_prob'] for d in digits]\n",
    "    plt.bar(x - width/2, original_probs, width, label='Original', alpha=0.7)\n",
    "    plt.bar(x + width/2, forgotten_probs, width, label='Forgotten', alpha=0.7)\n",
    "    for fd in forgotten_digits:\n",
    "        if fd in digits:\n",
    "            plt.axvline(x=digits.index(fd), color='red', linestyle='--')\n",
    "    plt.xlabel('Digit')\n",
    "    plt.ylabel('Target Probability')\n",
    "    plt.title('Target Probability Comparison')\n",
    "    plt.xticks(x, digits)\n",
    "    plt.legend([f'Forgotten Digits: {forgotten_digits}'])\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    entropy_diff = [f - o for o, f in zip(original_entropies, forgotten_entropies)]\n",
    "    colors = ['red' if d in forgotten_digits else 'blue' for d in digits]\n",
    "    plt.bar(x, entropy_diff, color=colors, alpha=0.7)\n",
    "    plt.xlabel('Digit')\n",
    "    plt.ylabel('Entropy Difference (Forgotten - Original)')\n",
    "    plt.title('Entropy Difference')\n",
    "    plt.xticks(x, digits)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    prob_diff = [o - f for o, f in zip(original_probs, forgotten_probs)]\n",
    "    colors = ['red' if d in forgotten_digits else 'blue' for d in digits]\n",
    "    plt.bar(x, prob_diff, color=colors, alpha=0.7)\n",
    "    plt.xlabel('Digit')\n",
    "    plt.ylabel('Probability Difference (Original - Forgotten)')\n",
    "    plt.title('Probability Difference')\n",
    "    plt.xticks(x, digits)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.exp_root_dir, 'comparison_analysis.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def display_sample_grids(config):\n",
    "    fig, axes = plt.subplots(2, 10, figsize=(20, 4))\n",
    "    for digit in range(10):\n",
    "        orig_grid_path = os.path.join(config.sample_dir, f'original_digit_{digit}_grid.png')\n",
    "        if os.path.exists(orig_grid_path):\n",
    "            orig_img = Image.open(orig_grid_path)\n",
    "            axes[0, digit].imshow(orig_img)\n",
    "            axes[0, digit].set_title(f'Original {digit}')\n",
    "            axes[0, digit].axis('off')\n",
    "\n",
    "        forg_grid_path = os.path.join(config.sample_dir, f'forgotten_digit_{digit}_grid.png')\n",
    "        if os.path.exists(forg_grid_path):\n",
    "            forg_img = Image.open(forg_grid_path)\n",
    "            axes[1, digit].imshow(forg_img)\n",
    "            axes[1, digit].set_title(f'Forgotten {digit}')\n",
    "            axes[1, digit].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.exp_root_dir, 'sample_comparison_grid.png'), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def main(i=[0]):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    config = Config(i)\n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"Digit to forget: {config.digit_to_forget}\")\n",
    "    print(f\"Lambda (EWC): {config.lmbda}\")\n",
    "    print(f\"Gamma: {config.gamma}\")\n",
    "    print(f\"Forgetting iterations: {config.n_forget_iters}\")\n",
    "    print(f\"Load existing models: {config.load_existing_models}\")\n",
    "    print(f\"Output directory: {config.exp_root_dir}\")\n",
    "\n",
    "    if config.load_existing_models:\n",
    "        original_vae = load_model(OneHotCVAE, 'original_vae.pt', config, device)\n",
    "    else:\n",
    "        original_vae = None\n",
    "\n",
    "    if original_vae is None:\n",
    "        original_vae = train_cvae(config, device)\n",
    "    else:\n",
    "        print(\"Using pre-trained original VAE\")\n",
    "\n",
    "    if config.load_existing_models:\n",
    "        fisher_dict = load_fisher_matrix(config)\n",
    "    else:\n",
    "        fisher_dict = None\n",
    "\n",
    "    if fisher_dict is None:\n",
    "        fisher_dict = calculate_fim(original_vae, config, device)\n",
    "    else:\n",
    "        print(\"Using pre-calculated Fisher matrix\")\n",
    "\n",
    "    generate_samples(original_vae, config, device, \"original\")\n",
    "\n",
    "    forgotten_model_name = f'forgotten_vae_digit_{config.digit_to_forget}.pt'\n",
    "    if config.load_existing_models:\n",
    "        forgotten_vae = load_model(OneHotCVAE, forgotten_model_name, config, device)\n",
    "    else:\n",
    "        forgotten_vae = None\n",
    "\n",
    "    if forgotten_vae is None:\n",
    "        forgotten_vae, losses = train_forgetting(original_vae, fisher_dict, config, device)\n",
    "    else:\n",
    "        print(f\"Using pre-trained forgetting model for digit {config.digit_to_forget}\")\n",
    "        losses = []\n",
    "\n",
    "    generate_samples(forgotten_vae, config, device, \"forgotten\")\n",
    "\n",
    "    if config.load_existing_models:\n",
    "        classifier = load_classifier(config, device)\n",
    "    else:\n",
    "        classifier = None\n",
    "\n",
    "    if classifier is None:\n",
    "        classifier = train_classifier(config, device)\n",
    "    else:\n",
    "        print(\"Using pre-trained classifier\")\n",
    "\n",
    "    results = evaluate_with_classifier(classifier, config.sample_dir, device)\n",
    "    create_comparison_plots(results, config)\n",
    "    display_sample_grids(config)\n",
    "    print(\"\\nRESULTS SUMMARY\")\n",
    "    forgotten_digit = config.digit_to_forget\n",
    "    orig_key = f'original_digit_{forgotten_digit}'\n",
    "    forg_key = f'forgotten_digit_{forgotten_digit}'\n",
    "\n",
    "    if orig_key in results and forg_key in results:\n",
    "        print(f\"\\nForgotten Digit {forgotten_digit}:\")\n",
    "        print(f\"  Original - Entropy: {results[orig_key]['entropy']:.4f}, Target Prob: {results[orig_key]['target_prob']:.4f}\")\n",
    "        print(f\"  Forgotten - Entropy: {results[forg_key]['entropy']:.4f}, Target Prob: {results[forg_key]['target_prob']:.4f}\")\n",
    "        print(f\"  Entropy Change: {results[forg_key]['entropy'] - results[orig_key]['entropy']:.4f}\")\n",
    "        print(f\"  Probability Change: {results[orig_key]['target_prob'] - results[forg_key]['target_prob']:.4f}\")\n",
    "\n",
    "    print(f\"\\nAll results and visualizations saved to: {config.exp_root_dir}\")\n",
    "    print(f\"All models saved to: {config.base_model_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for i in range (10):\n",
    "        main(i)     # forget each digit\n",
    "    main([3, 7])    # example of multiple forgeting"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4424.588213,
   "end_time": "2025-10-23T12:17:08.458273",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-23T11:03:23.870060",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
